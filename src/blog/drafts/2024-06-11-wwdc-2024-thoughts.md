---
title: Thoughts on the WWDC 2024 keynote
date: 2024-06-11
tags: [technology]
---

I like Apple's Worldwide Developer Conference (more commonly named WWDC and more fondly "dub dub" by enthusiasts).

21st century Apple may have made their name as the benchmark for high quality industrial hardware design, but hardware is nothing without software, and WWDC is when Apple allows their software to take centre stage truly unencumbered.

WWDC is weeklong collection of talks and tutorials all about what is coming in Apple's ecosystem of products, these days all free and available online to anyone who wants to watch. But what will those talks and tutorials be about?

That's what the keynote is for. The typically 1Â½ to 2 hour event that kicks off WWDC, where Tim Cook and a host of random company vice presidents who you won't remember by next year introduce everything that WWDC will be about.

{% character character="ash", variant="love" %}
Forgettable except for Craig. We love Craig. ðŸ’–
{% endcharacter %}

It's a flying introduction, but one that I try to watch. It's kind of exciting in a weird way. I suppose that's to be expected when a year's worth of new features across half a dozen product lines gets announced all at once.

True to that style, here are some briefâ€”and not so briefâ€”notes on my initial thoughts.

(Of course I have a professional interest too, as WWDC includes what's changing in Safari, and that might impact what I do at work.)

## visionOS

We started with visionOS, Apple's latest addition to the family, getting the typical annual increment to visionOS 2.

Nothing here really caught my interest. I've never really gotten into virtual or augmented reality stuff, though I know plenty of people who are.

I do wish I had a massive, floating, wraparound MacBook screen though.

## watchOS

This section felt quite thin to me, but that probably speaks more to how I use my Apple Watch more than anything else. I'm not someone who works out frequent and I have a very low risk of getting pregnant, so the two new flagship app features aren't exactly in my wheelhouse.

Still, I appreciate the Watch getting contextual Live Activity support. I just wish more apps actually implemented them, still.

The Watch also got the funky photo and text customisation features from the iPhone lock screen, giving a bit more possibility for on-the-fly watch face customisation. I don't tend to use photos for my Apple Watch face, but it might be fun to play around with a bit.

## Audio & Home

I have three HomePods and two Apple TVs in my flat, all of which I use quite a lot, so hurray for a section that interested me!

Alas, updates here were pretty scant. The only update on the Home front with any relevance was the addition of HomeKit support for robot vacuums. Will Roombhilda, my Roomba, ever be updated to have it? I kinda doubt it.

Updates to Apple TV were a little more interesting.

The InSight feature, which provides information about the information on screen, currently playing songs and other metadata (Sherlocking Amazon Prime Video's X-Ray) is something I always found it weird that Apple TV didn't do before, especially as the TV obviously had all of that data sequestered in other places. It's nice that it's there now.

Also of interest to me was a new feature to dynamically enhance spoken voices in videos.

I am... bad at hearing. [I have problems with audio processing]({{ '/about/neurodivergent/' | url }}) in loud environments and am partially deaf in one ear to boot. I am one of those supposedly weird millenials who [watches everything with subtitles](https://www.businessinsider.com/generation-z-subtitles-closed-captioning-millenials-no-hearing-loss-2023-8?op=1).

Whilst ostensibly an accessibility feature (and one I'll probably use!) this also just struck me as an indightment of the often muddy audio mixing that pervades modern, prestige television. Everything sounds bad and this is how we're being forced to fix it. Fun.

## iOS

Arguably Apple's flagship operating system, and certainly their most used.

This section also ended up feeling very thin, but in hindsight that's probably because they were saving so much of it until the last part of the keynote.

The Photos app is getting a fairly major redesign, with the tabbed interface being abolished for some weird hybrid of the recent photos and for you viewsâ€”an infinite scrolling screen that starts you in the middle, swipe down to view your photos in reverse chronologicla order, swipe up to navigate through carousel after carousel of automatic and manually created albums and collections.

I'm not a fan. This is so reminiscent of an infinite scrolling feed, the kind that pervades the kind of social media websites desperate to hook a person's attention and never let it go. Finding the right thing involves swiping across two dimensions.

It feels like it's been optimised heavily towards idle browsing. I've yet to use it, of course, but I can imagine it'll be be so much slower to use if you know what you're looking for, compared to the current tabbed interface.

The default Mail app is getting dynamically filtered inboxes, similar to Gmail and Google's now defunct Inbox app. It's a small thing, but it's nice to have.

There are some changes to the home screen, allowing users to finally break out of Apple's top-left oriented grid and put icons pretty much anywhere. Icons can now be dark or tinted a custom colour too, with widgets following suit.

This is fine for the most part, but I think the tinted icons look **so bad**. Many of them just don't adapt well to being forcibly recoloured, and this is especially problematic on the widgets, where complex graphics are more common and colour may be used to convey context.

This definitely needs work. Apple should probably take a leaf out of Google's book here and switch to using [maskable icons](https://web.dev/articles/maskable-icon) that are designed for that purpose instead of trying to override the colours.

## iPadOS

iPadOS gets many of the same updates as iOS, so I won't bother to reiterate them here.

The big news here is that, after a decade of lacking one [due to Steve Jobs' distaste for it](https://www.youtube.com/watch?v=WK_AtW66XI0), the iPad finally got a calculator app.

And it's... kinda really cool. Aside from calculations (obviously) it does unit conversions too, and has Notes-app like workspaces with handwriting support. These workspaces utilise optical character recognition (OCR) to recognise equations and provide the answers for them inline in a emulation of your own handwriting. This works for algebra, it works for graphs, it works for all the funky scientific calculator functions.

It works surprisingly well; maybe even a decades worth of waiting well. All it seems to lack is some natural language processing. It'd be nice to be able to write '15% of 125' to get an answer rather than the more mathsy '125 Ã— 1.15'. Maybe next year.

Pretty much all of this functionality has been duplicated to the Notes app, which has had it's OCR capabilities massively boosted. Handwritten text is now basically indistinguishable from typed text. It reflows across lines, can be spell checked, be inserted, deleted, formatted, and all that good stuff; all whilst emulating the style of the hand that wrote it.

Well... I was impressed with that, anyway.

## macOS

If they announced anything new for macOS then I probably forgot most of it, because I was totally enamoured by the latest feature in the Continuity family: iPhone mirroring.

That is, you can use your iPhone from your Mac. Securely. Wirelessly. The whole thing. Every app and feature.

This isn't just some remote access program either. iPhone notifications are synced to the Mac as well, and clicking one of those connects to the iPhone right then and there, opening the relevant app as though you'd tapped the actual notification on your phone.
You can drag files back and forth between iPhone and Mac too, hopefully reducing the need for me to randomly AirDrop files back and forth.

As a perennial leaver of my iPhone in the other room until I inevitably need it for one small thing and have to go and get it, this is actually a game changer. Now I can do a quick thing without having to get out of bed. Game changing.

Also new to macOS is edge-based window tiling. I know **so many** people who complained about Macs not having this feature, for which my response has always been to point [at Microsoft's patent over the concept](https://patents.google.com/patent/US10592080B2/en) and telling them to complain to Microsoft. It seems Apple finally caved and paid the license fee for it.

Safari also gets some neat updates, amongst them a 'highlights' feature that can pull particular semantic data out of the content of webpagesâ€”such as people, locations, and songsâ€”and present these separately with their own shortcuts.

Hmm, that sounds a lot like something a large language model would do... surely not?

## Apple Intelligence

The last third of the keynote focused entirely on Apple's homegrown competitor to the likes of ChatGPT, Google Gemini and Microsoft Copilotâ€”the very Apple-branded Apple Intelligence.

Apple Intelligence doesn't use a chat-like interface, but it is _everywhere_ in the new software updates. The list of things they mentioned using it would be pretty long, so here's the ones I took particular note of:

- The aforementioned highlights feature in Safari.
- Mail app's inbox showing summaries of email content instead of the first few lines.
- Mail app parsing the content of emails to recognise if an email should be marked as priority, or marked as priority at some point later.
- 'Proofreading', basically spelling and grammar checking with added suggestions for rewording stuff.
- Squashing large numbers of unread notifications from the same source (such as a group chat) into a single summary notification.
- Intelligent do not disturb modes that only allows notifications if Apple Intelligence deems them high priority enough.

Potentially controversial opinion, but these are things that I don't mind large language models (LLMs, the actual name of what many companies call "AI") being used for. These are tasks that would normally involve sorting through noise, an investment of time, and manual curation.

Y'know, the kind of boring crap that automation was supposed to get rid of.

This new ability to parse and understand context apparently also gives Siri superpowers, supposedly soon to be able to perform hyper-specific, hyper-contextual tasks across multiple apps in a single command. This is basically what everyone wanted Siri to become when it was first introduced, so I look forward to seeing how it works out.

{% character character="ash", variant="tongue" %}
I find it kinda funny how Apple managed to Sherlock [the much derided Rabbit R1's killer feature](https://www.youtube.com/watch?v=ddTV12hErTc&t=596s) and seemingly do a much better job at it before Rabbit even managed to fully launch it.
{% endcharacter %}

Much less cool, in my opinion, is that Apple are bundling the same text and image synthesis tools that every other LLM tool also has.

They don't exist to reduce cruft or tedium, they exist to shortcut individual creativity and to replace real effort and soul with algorithmically-generated drivel.

Suffice to say, I'm not a fan.

Perhaps the only saving grace for this is that it's Apple's version of it. OpenAI's ChatGPT and Dall-E are widely acknowledged to have been trained on copywritten dataâ€”essentially stealing that which is provided freely to create a product that OpenAI charges for.

This doesn't appear to be the case for Apple's homegrown LLM, which appears to be limited in the complexity of what it can create, implying a much smaller, much more focused data set was used for training, bypassing some of the common ethical complaints of using LLMs.

The other common complaintâ€”the high environmental cost of LLM data centresâ€”also seems to be well suited to Apple's hand. Apple Intelligence does as much as possible on the user's device, only falling back to remote servers if necessary, and the company frequently boasts about their data centres being completely powered by renewable energy.

This is not a perfect outcome. An LLM server being used is still going to use more carbon and more power than one that was never built in the first place, but on the surface it's more than every major competitor has committed to.

Apple then kinda ruined it by immediately announcing that ChatGPT would be the first 'plug-in' to their AI ecosystem, but... well, I suppose if they did otherwise people would just complain about Apple restricting choice or something. Ho hum.

## Overview
